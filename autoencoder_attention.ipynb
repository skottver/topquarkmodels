{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3217244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "import torch.cuda.amp as amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c67fc955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/WW.Flz.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/WW.Flz.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part3.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part4.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 0 /scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part5.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:50<00:00,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:06, 66.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 1 /scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_tbar_4f.leB.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 1 /scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_tbar_4f.leB.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 1 /scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_top_4f.qlh.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 1 /scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_top_4f.qlh.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:48<00:00, 12.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [02:17, 69.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 2 /scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 2 /scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 2 /scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part3.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 2 /scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part4.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 2 /scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_semilep.vRt.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 2 /scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_semilep.vRt.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:14<00:00, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:05, 86.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 3 /scratch/pvolkov/DM/samples/MultiLepton/grid/TTTJ_UL16Summer20_RoO.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 3 /scratch/pvolkov/DM/samples/MultiLepton/grid/TTTW_UL16Summer20_CgT.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "4it [04:08, 53.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 4 /scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part1.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 4 /scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part2.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 4 /scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part3.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n",
      "processing 4 /scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part4.root\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:35<00:00,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [04:53, 58.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (8198428, 126)\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "# #paths to samples\n",
    "zero_t_samples = [\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/WW.Flz.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/WW.Flz.part2.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part2.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part3.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part4.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/Wjets-incl.INb.part5.root',\n",
    "]\n",
    "\n",
    "one_t_samples = [\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_tbar_4f.leB.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_tbar_4f.leB.part2.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_top_4f.qlh.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/t-channel_top_4f.qlh.part2.root',\n",
    "]\n",
    "\n",
    "two_t_samples = [\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part2.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part3.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_doublelep.KQP.part4.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_semilep.vRt.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/ttbar_semilep.vRt.part2.root',\n",
    "]\n",
    "\n",
    "three_t_samples = [\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/TTTJ_UL16Summer20_RoO.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/TTTW_UL16Summer20_CgT.root',\n",
    "]\n",
    "\n",
    "four_t_samples = [\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part1.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part2.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part3.root',\n",
    "    '/scratch/pvolkov/DM/samples/MultiLepton/grid/TTTT_UL16Summer20_eIE.part4.root',\n",
    "]\n",
    "\n",
    "# create dataframe\n",
    "total_data = []\n",
    "for i, samples in tqdm(enumerate([zero_t_samples,one_t_samples,two_t_samples,three_t_samples,four_t_samples])):\n",
    "    list_of_samples = []\n",
    "    for path in tqdm(samples):\n",
    "        print('processing',i, path)\n",
    "        file = uproot.open(path)\n",
    "        a = file['LHE'].arrays(file['LHE'].keys(), library=\"np\")\n",
    "        df = pd.DataFrame(a)\n",
    "        df = df.loc[:, ~df.columns.str.startswith('SP_')]\n",
    "        list_of_samples.append(df)\n",
    "        print('successful')\n",
    "    df_sample = pd.concat(list_of_samples).reset_index()\n",
    "    df_sample['class'] = i\n",
    "    total_data.append(df_sample)\n",
    "data = pd.concat(total_data).reset_index()\n",
    "data = data.drop(columns = ['level_0', 'index'])\n",
    "print('shape:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bdd9f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "particle_types = {\n",
    "    \"BJet\": 4,\n",
    "    \"Jet\": 12,\n",
    "    \"Lep\": 4\n",
    "}\n",
    "\n",
    "particle_features = [\"Pt\", \"Eta\", \"Phi\", \"Px\", \"Py\", \"Pz\"]\n",
    "\n",
    "particle_cols = []\n",
    "for p_type, count in particle_types.items():\n",
    "    for i in range(1, count + 1):\n",
    "        for feat in particle_features:\n",
    "            col_name = f\"{feat}_{p_type}{i}\"\n",
    "            particle_cols.append(col_name)\n",
    "\n",
    "global_cols = [\"N_J\", \"N_BJ\", \"N_Nu\", \"N_Lep\", \"MeT\"]\n",
    "\n",
    "label_col = \"class\"\n",
    "\n",
    "train_df, val_df = train_test_split(data, test_size=0.2, stratify=data['class'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7b4c94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AEDataset(Dataset):\n",
    "    def __init__(self, df, particle_cols, global_cols, num_particles=20, particle_feat_dim=6):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.particle_cols = particle_cols\n",
    "        self.global_cols = global_cols\n",
    "        self.num_particles = num_particles\n",
    "        self.particle_feat_dim = particle_feat_dim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        \n",
    "        particles = torch.tensor(row[self.particle_cols].values.reshape(self.num_particles, self.particle_feat_dim), dtype=torch.float32)\n",
    "\n",
    "        \n",
    "        global_feats = torch.tensor(row[self.global_cols].values, dtype=torch.float32)\n",
    "\n",
    "        return particles, global_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d7f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AEEncoder(nn.Module):\n",
    "    def __init__(self, num_particles=20, particle_feat_dim=6, global_feat_dim=5, emb_dim=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.particle_proj = nn.Linear(particle_feat_dim, emb_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=emb_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.particle_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.global_net = nn.Sequential(\n",
    "            nn.Linear(global_feat_dim, emb_dim),\n",
    "            nn.LayerNorm(emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "        self.combined_proj = nn.Linear(emb_dim * 2, emb_dim)\n",
    "\n",
    "    def forward(self, particles, global_feats):\n",
    "        x = self.particle_proj(particles)\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        pooled_particles = self.particle_pool(attn_out.transpose(1, 2)).squeeze(-1)\n",
    "        global_emb = self.global_net(global_feats)\n",
    "        combined = torch.cat([pooled_particles, global_emb], dim=1)\n",
    "        emb = self.combined_proj(combined)\n",
    "        return emb\n",
    "\n",
    "class AEDecoder(nn.Module):\n",
    "    def __init__(self, num_particles=20, particle_feat_dim=6, global_feat_dim=5, emb_dim=64):\n",
    "        super().__init__()\n",
    "        output_dim = num_particles * particle_feat_dim + global_feat_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "        self.num_particles = num_particles\n",
    "        self.particle_feat_dim = particle_feat_dim\n",
    "        self.global_feat_dim = global_feat_dim\n",
    "\n",
    "    def forward(self, emb):\n",
    "        out = self.decoder(emb)\n",
    "        particles = out[:, :self.num_particles * self.particle_feat_dim]\n",
    "        global_feats = out[:, self.num_particles * self.particle_feat_dim:]\n",
    "        particles = particles.view(-1, self.num_particles, self.particle_feat_dim)\n",
    "        return particles, global_feats\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, num_particles=20, particle_feat_dim=6, global_feat_dim=5, emb_dim=64, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.encoder = AEEncoder(num_particles, particle_feat_dim, global_feat_dim, emb_dim, num_heads)\n",
    "        self.decoder = AEDecoder(num_particles, particle_feat_dim, global_feat_dim, emb_dim)\n",
    "\n",
    "    def forward(self, particles, global_feats):\n",
    "        emb = self.encoder(particles, global_feats)\n",
    "        particles_recon, global_recon = self.decoder(emb)\n",
    "        return particles_recon, global_recon, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08140fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AEDataset(train_df, particle_cols, global_cols, num_particles=20, particle_feat_dim=6)\n",
    "val_dataset = AEDataset(val_df, particle_cols, global_cols, num_particles=20, particle_feat_dim=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3803c001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(particles_recon, particles_true, global_recon, global_true):\n",
    "    loss_particles = F.mse_loss(particles_recon, particles_true)\n",
    "    loss_global = F.mse_loss(global_recon, global_true)\n",
    "    return loss_particles + loss_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fdfe856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, train_dataset, val_dataset, config):\n",
    "    device = config[\"device\"]\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=6,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=6,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "        for batch in loop:\n",
    "            particles, global_feats = batch  # вот здесь батч\n",
    "            particles = particles.to(device, non_blocking=True)\n",
    "            global_feats = global_feats.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            particles_recon, global_recon, emb = model(particles, global_feats)\n",
    "            loss = loss_fn(particles_recon, particles, global_recon, global_feats)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                particles, global_feats = batch\n",
    "                particles = particles.to(device, non_blocking=True)\n",
    "                global_feats = global_feats.to(device, non_blocking=True)\n",
    "\n",
    "                particles_recon, global_recon, emb = model(particles, global_feats)\n",
    "                loss = loss_fn(particles_recon, particles, global_recon, global_feats)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"[{epoch+1}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), config[\"model_save_path\"])\n",
    "            print(\">> Best model saved.\")\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Autoencoder Training Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config[\"loss_plot_path\"])\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d05a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder(\n",
       "  (encoder): AEEncoder(\n",
       "    (particle_proj): Linear(in_features=6, out_features=64, bias=True)\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (particle_pool): AdaptiveAvgPool1d(output_size=1)\n",
       "    (global_net): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "      (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (combined_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): AEDecoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Linear(in_features=256, out_features=125, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 20,\n",
    "    \"model_save_path\": \"best_ae_model.pt\",\n",
    "    \"loss_plot_path\": \"ae_loss_curve.png\"\n",
    "}\n",
    "model = Autoencoder(\n",
    "    num_particles=20,\n",
    "    particle_feat_dim=6,\n",
    "    global_feat_dim=5,\n",
    "    emb_dim=64,\n",
    "    num_heads=4\n",
    ")\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d670aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   1%|          | 557/51241 [00:28<43:26, 19.44it/s, loss=4.45e+3]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, train_dataset, val_dataset, config)\u001b[0m\n\u001b[1;32m     26\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     27\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 29\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_feats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# вот здесь батч\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparticles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib64/python3.12/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib64/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_autoencoder(model, train_dataset, val_dataset, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
